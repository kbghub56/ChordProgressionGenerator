{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a09f64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "722de7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"songSectionDataClean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3a8ecf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I-VI#-ii-I-v-ii-I', 'VI#-ii-I-v-ii-I', 'v-ii-I', 'I-VI#-ii-I-v-ii-I', 'I-VI#-ii-I-v-ii-I', 'I-VI#-ii-I-v-ii-I', 'I-ii-vi-IV', 'I-ii-vi-IV', 'I-ii-vi-IV', 'iii-vi-V', 'I-ii-vi-IV', 'I-vi-IV-V', 'vi-IV-I-V', 'I-V-vi-IV', 'vi-IV-I-V-I-V', 'vi', 'vi', 'vi-I-IV-III', 'I-V-ii-iv', 'vi-V#-I-II-ii', 'ii-V-vi-V#-I-IV#', 'I-V-ii', 'vi', 'vi-ii-III-vi', 'vi', 'vi-ii-III-vi', 'vi-ii-III-vi', 'I-II', 'ii-vi-I-v', 'ii-i#-I#-I-ii-vi#-VII-ii-vi#-VII-vi#-ii-vi#-VII-ii', 'I-II', 'I-II-iii-V-I', 'I-II-iii', 'I-II-iii', 'I-vi-ii-I', 'IV-vi-ii-I', 'IV-vi-ii-I', 'IV-vi-ii-I', 'I-V-vi-IV-I', 'I-V-vi-IV', 'I-V-vi-IV-I-V-vi-V', 'I-V-vi-IV', 'IV-vi-V-iii', 'IV-vi-V-iii', 'iii-IV-vi-V', 'IV-vi-V', 'IV-vi-V-iii', 'I-I#', 'I-I#', 'ii-V-I-iii-vi', 'I-I#', 'I-V-vi-V', 'I-V-vi-V-I-V-vi-II', 'I-V-vi-VII-I-V-vi-II-I-VII-iii', 'I', 'I-V-ii-IV', 'I-V-ii-IV', 'vi-iii-IV-ii', 'vi-iii-IV-ii', 'vi-iii-IV-ii', 'IV-III-vi-I', 'IV-III-vi-I', 'IV-III-vi-I-IV-III-vi', 'ii-V-I-IV', 'I-V-ii-IV', 'I-V-ii-IV', 'I-V-ii-IV-V', 'ii-IV-I-V', 'VI#-I-ii-I-IV-V', 'I-V-ii-I-IV', 'I-ii-IV-V', 'I-ii-IV-V', 'I-ii-IV-V', 'I-ii-IV-V', 'ii-IV-I-V', 'I-vi-IV', 'I-vi-IV-I-V', 'vi-V-I-IV-vi-V-IV', 'I-iii-IV-I-V-I', 'I-V-IV-vi-V', 'I-vi-IV', 'vi-IV-I-V', 'I-iii-IV-I-V', 'I-vi-IV', 'vi-I-V-II', 'vi-I-V-II', 'V-II-vi-I', 'vi-I-V-II', 'vi-I-V-II', 'vi-I-V-II', 'vi-I-V-II', 'I#', 'I#', 'I#', 'I#-vi#-V#', 'ii-vi', 'vi', 'vi-iii-ii-vi', 'vi', 'I', 'I-V-vi-IV-V', 'IV-iii-IV-iii-V', 'I-vi-IV-V', 'IV-vi-V-ii', 'I-vi-IV-V', 'IV-vi-I-ii-vi-IV-vi-V-I-IV', 'IV-vi-I-ii-vi-IV-vi-V-I-IV', 'I-IV-vi-I-ii-vi-IV-vi-V-I', 'I-ii-vi-IV-vi-V-I-IV', 'I', 'I-iii-V-I', 'I-iii-V-I-vi-iii-V-I-vi-iii-V-I', 'vi-iii-V-I', 'vi-IV-ii-vi', 'vi-IV-ii-vi', 'vi-IV-ii-vi', 'vi', 'I-V-vi-IV', 'I-V-vi-IV', 'I-V-vi-IV', 'I-V-vi-IV', 'vi-II-V', 'vi-II-V', 'vi-II-V', 'vi-II-V', 'V-vi-II-V', 'III-VII-IV#-v#', 'IV#-v#-III-VII', 'vi-IV-III', 'vi-V-IV-III', 'vi-V-IV-III', 'vi-V-IV-III', 'vi-IV-III', 'ii-vi-IV-I', 'ii-vi-IV-I', 'IV-I-V', 'IV-I-vi-V', 'IV-I-V', 'I-V-vi-ii-I-V', 'vi-IV-I', 'I-V-vi-IV-I-V-I', 'III', 'I', 'I-V-ii-IV', 'ii-IV-I-V-ii-IV-V', 'I-V-ii-IV-I', 'I-V-ii-IV-ii-IV-I-V-ii-IV', 'V-ii-IV-I', 'II#-v-IV', 'II#-v-IV', 'II#-v-IV', 'II#-v-IV', 'II#-v-IV', 'II#-v-IV', 'I-V-vi-IV', 'I-IV-vi-IV-I-IV-vi-V', 'IV-V-vi-I-IV-V-vi', 'I-V-vi-IV-V-I', 'I-V-vi-IV-V', 'vi-IV-I-V', 'II-VI-vii-V-VI-II', 'I-ii-V', 'I-ii-V', 'III-VII-iv#-vi', 'III-VII-iv#-vi', 'III-VII-i#-VI-vi', 'III-VII-iv#-vi-III', 'IV', 'I-ii-V-I-vi-ii-IV-V-I', 'I-vi-IV#-IV-V', 'I-ii-IV-V-I', 'VIIb-IV-I-V-IIIb-VIIb-IV-V', 'I-ii-IV-V-I-vi-IV-I', 'VI#', 'VI#-IV-i-II#', 'i-II#-VI#-IV-i-II#-IV', 'VI#-IV-i-II#-VI#', 'VI#-IV-i-II#-i-II#-VI#-IV-i-II#', 'IV-i-II#-VI#', 'vi-IV-ii-III', 'vi-IV-ii-III', 'vi-IV-ii-III', 'vi-IV-ii-III', 'vi-IV-ii-III', 'vi-V-ii-iii', 'vi-I-V', 'vi-I-V-IV', 'vi-I-V-IV', 'I-III-vii-V', 'III-I-V-I', 'III-vii-III', 'I-iii-ii-iii-ii', 'I-iii-ii-iii-ii', 'I-iii-ii-iii-ii', 'I-iii-ii-iii-ii', 'vi-ii-III', 'vi-ii-III', 'vi-ii-III', 'vi-ii-III', 'vi', 'vi', 'I-iii-II', 'I-iii-II', 'V-II-vi-I', 'V-II-vi-I', 'I-v-IV', 'I-v-IV', 'I-v-IV', 'V#-II#-IV-V#-VI#-I', 'I', 'vi-II-V-iii', 'vi-II-V-iii', 'vi-II-V-iii', 'I-V-vi-IV', 'I-V-vi-IV', 'I-V-vi-IV', 'V-vi-IV-I-V-vi-IV', 'I-V-vi-IV', 'I-V-vi-IV', 'IV-I', 'IV-I', 'IV-ii-V-vi-I-IV-I', 'ii-V-IV', 'IV-ii-V-vi-I-IV-I-IV-I-IV-I', 'vi-V-IV', 'V-IV-vi', 'vi', 'V-IV-vi-V', 'vi', 'I-vi-IV-V-I-vi-IV-V-IV-V', 'I-vi-ii-IV-I-vi-ii-IV-I', 'I', 'I-IV-III-vi', 'I-IV-III-vi', 'I-IV-III-vi', 'IV-I', 'V-I-iii-I', 'V-I-iii-I', 'V-iii-I-II', 'iii-II-I-V-II-I-V-II-I-IV', 'I-iii-I-V', 'vii-VI', 'vii-VI', 'II-VI-vii-V', 'vii-VI', 'vi-IV-I-III-vi-IV-I', 'vi-IV-I-III', 'I-IV-iv-I-IV-ii-vi-IV-I-III', 'V#-VIIb', 'I', 'I', 'I', 'I', 'I-IV']\n"
     ]
    }
   ],
   "source": [
    "data = df.Progression.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2cd67a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters=' ', split=\"-\")  # chords are separated by '-'\n",
    "tokenizer.fit_on_texts(data)\n",
    "\n",
    "# convert chord progressions to sequences of integer tokens\n",
    "sequences = tokenizer.texts_to_sequences(data)\n",
    "\n",
    "# create input and output sequences\n",
    "input_sequences = []\n",
    "output_sequences = []\n",
    "for sequence in sequences:\n",
    "    for i in range(1, len(sequence)):\n",
    "        input_sequences.append(sequence[:i])\n",
    "        output_sequences.append(sequence[i])\n",
    "\n",
    "# pad input sequences with zeros at the beginning\n",
    "input_sequences = pad_sequences(input_sequences)\n",
    "\n",
    "# convert output sequences to categorical (one-hot encoding)\n",
    "output_sequences = to_categorical(output_sequences, num_classes=len(tokenizer.word_index) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8832e46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=input_sequences.shape[1]))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dense(len(tokenizer.word_index) + 1, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84a6bb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "26/26 [==============================] - 3s 39ms/step - loss: 2.2436 - accuracy: 0.1799\n",
      "Epoch 2/50\n",
      "26/26 [==============================] - 1s 48ms/step - loss: 2.0644 - accuracy: 0.1861\n",
      "Epoch 3/50\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 2.0774 - accuracy: 0.1973\n",
      "Epoch 4/50\n",
      "26/26 [==============================] - 2s 58ms/step - loss: 2.0479 - accuracy: 0.2084\n",
      "Epoch 5/50\n",
      "26/26 [==============================] - 2s 68ms/step - loss: 2.0400 - accuracy: 0.2035\n",
      "Epoch 6/50\n",
      "26/26 [==============================] - 1s 57ms/step - loss: 2.0081 - accuracy: 0.2643\n",
      "Epoch 7/50\n",
      "26/26 [==============================] - 2s 59ms/step - loss: 1.9402 - accuracy: 0.3040\n",
      "Epoch 8/50\n",
      "26/26 [==============================] - 1s 54ms/step - loss: 1.8081 - accuracy: 0.3573\n",
      "Epoch 9/50\n",
      "26/26 [==============================] - 1s 56ms/step - loss: 1.7198 - accuracy: 0.4045\n",
      "Epoch 10/50\n",
      "26/26 [==============================] - 2s 61ms/step - loss: 1.6400 - accuracy: 0.4231\n",
      "Epoch 11/50\n",
      "26/26 [==============================] - 1s 55ms/step - loss: 1.5752 - accuracy: 0.4318\n",
      "Epoch 12/50\n",
      "26/26 [==============================] - 2s 59ms/step - loss: 1.5423 - accuracy: 0.4417\n",
      "Epoch 13/50\n",
      "26/26 [==============================] - 2s 62ms/step - loss: 1.4594 - accuracy: 0.4578\n",
      "Epoch 14/50\n",
      "26/26 [==============================] - 2s 59ms/step - loss: 1.4257 - accuracy: 0.4963\n",
      "Epoch 15/50\n",
      "26/26 [==============================] - 1s 55ms/step - loss: 1.3483 - accuracy: 0.5298\n",
      "Epoch 16/50\n",
      "26/26 [==============================] - 1s 53ms/step - loss: 1.2941 - accuracy: 0.5397\n",
      "Epoch 17/50\n",
      "26/26 [==============================] - 2s 59ms/step - loss: 1.2459 - accuracy: 0.5608\n",
      "Epoch 18/50\n",
      "26/26 [==============================] - 2s 69ms/step - loss: 1.2121 - accuracy: 0.5546\n",
      "Epoch 19/50\n",
      "26/26 [==============================] - 2s 67ms/step - loss: 1.1530 - accuracy: 0.5806\n",
      "Epoch 20/50\n",
      "26/26 [==============================] - 2s 68ms/step - loss: 1.0886 - accuracy: 0.5980\n",
      "Epoch 21/50\n",
      "26/26 [==============================] - 2s 61ms/step - loss: 1.0478 - accuracy: 0.6092\n",
      "Epoch 22/50\n",
      "26/26 [==============================] - 1s 53ms/step - loss: 1.0114 - accuracy: 0.6266\n",
      "Epoch 23/50\n",
      "26/26 [==============================] - 2s 60ms/step - loss: 0.9945 - accuracy: 0.6328\n",
      "Epoch 24/50\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 0.9511 - accuracy: 0.6538\n",
      "Epoch 25/50\n",
      "26/26 [==============================] - 2s 62ms/step - loss: 0.9273 - accuracy: 0.6538\n",
      "Epoch 26/50\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 0.9057 - accuracy: 0.6538\n",
      "Epoch 27/50\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 0.8891 - accuracy: 0.6638\n",
      "Epoch 28/50\n",
      "26/26 [==============================] - 2s 60ms/step - loss: 0.8448 - accuracy: 0.6836\n",
      "Epoch 29/50\n",
      "26/26 [==============================] - 2s 62ms/step - loss: 0.8625 - accuracy: 0.6799\n",
      "Epoch 30/50\n",
      "26/26 [==============================] - 2s 61ms/step - loss: 0.8423 - accuracy: 0.6787\n",
      "Epoch 31/50\n",
      "26/26 [==============================] - 2s 62ms/step - loss: 0.8412 - accuracy: 0.6762\n",
      "Epoch 32/50\n",
      "26/26 [==============================] - 2s 61ms/step - loss: 0.8226 - accuracy: 0.6811\n",
      "Epoch 33/50\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 0.7976 - accuracy: 0.6861\n",
      "Epoch 34/50\n",
      "26/26 [==============================] - 2s 66ms/step - loss: 0.7967 - accuracy: 0.6911\n",
      "Epoch 35/50\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 0.7883 - accuracy: 0.6923\n",
      "Epoch 36/50\n",
      "26/26 [==============================] - 1s 55ms/step - loss: 0.7773 - accuracy: 0.6935\n",
      "Epoch 37/50\n",
      "26/26 [==============================] - 2s 68ms/step - loss: 0.7888 - accuracy: 0.6638\n",
      "Epoch 38/50\n",
      "26/26 [==============================] - 2s 71ms/step - loss: 0.7913 - accuracy: 0.6749\n",
      "Epoch 39/50\n",
      "26/26 [==============================] - 2s 68ms/step - loss: 0.7875 - accuracy: 0.6861\n",
      "Epoch 40/50\n",
      "26/26 [==============================] - 2s 68ms/step - loss: 0.7775 - accuracy: 0.6898\n",
      "Epoch 41/50\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 0.7746 - accuracy: 0.6911\n",
      "Epoch 42/50\n",
      "26/26 [==============================] - 2s 58ms/step - loss: 0.7735 - accuracy: 0.6749\n",
      "Epoch 43/50\n",
      "26/26 [==============================] - 2s 58ms/step - loss: 0.7657 - accuracy: 0.6911\n",
      "Epoch 44/50\n",
      "26/26 [==============================] - 2s 67ms/step - loss: 0.7541 - accuracy: 0.6948\n",
      "Epoch 45/50\n",
      "26/26 [==============================] - 2s 59ms/step - loss: 0.7493 - accuracy: 0.6998\n",
      "Epoch 46/50\n",
      "26/26 [==============================] - 2s 62ms/step - loss: 0.7610 - accuracy: 0.6824\n",
      "Epoch 47/50\n",
      "26/26 [==============================] - 2s 72ms/step - loss: 0.7472 - accuracy: 0.7022\n",
      "Epoch 48/50\n",
      "26/26 [==============================] - 2s 70ms/step - loss: 0.7446 - accuracy: 0.7010\n",
      "Epoch 49/50\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 0.7399 - accuracy: 0.6973\n",
      "Epoch 50/50\n",
      "26/26 [==============================] - 2s 59ms/step - loss: 0.7410 - accuracy: 0.6923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x16a8bedd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(input_sequences, output_sequences, epochs=50, verbose=1)  # choose suitable number of epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f936614f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "['v', 'vi']\n"
     ]
    }
   ],
   "source": [
    "def predict_chord(model, tokenizer, text, num_chords):\n",
    "    sequence = tokenizer.texts_to_sequences([text])[0]\n",
    "    sequence = pad_sequences([sequence], maxlen=input_sequences.shape[1])\n",
    "    \n",
    "    # Generate a sequence of chords\n",
    "    chord_sequence = []\n",
    "    for _ in range(num_chords):\n",
    "        prediction = model.predict(sequence)\n",
    "        predicted_class = np.argmax(prediction, axis=-1)\n",
    "        chord = tokenizer.sequences_to_texts([predicted_class])[0]\n",
    "        chord_sequence.append(chord)\n",
    "        \n",
    "        # Update sequence with predicted class for next prediction\n",
    "        sequence = np.append(sequence[0, 1:], predicted_class)\n",
    "        sequence = np.reshape(sequence, (1, len(sequence)))\n",
    "        \n",
    "    return chord_sequence\n",
    "\n",
    "print(predict_chord(model, tokenizer, 'I', 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8438c076",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
